# üñ•Ô∏è Self-Hosted AI Models Setup Guide

## FULLY LOCAL AI - NO EXTERNAL APIs

This guide sets up **completely self-hosted AI models** running on your own hardware with no reliance on external services.

---

## üéØ Architecture Overview

### What We're Building:

```
Your Application (Convex)
    ‚Üì HTTP Requests
Local AI Server (Your GPU Machine)
    ‚îú‚îÄ‚îÄ Ollama (Port 11434) - Text Generation
    ‚îú‚îÄ‚îÄ ComfyUI (Port 8188) - Image/Video Generation
    ‚îî‚îÄ‚îÄ CoquiTTS (Port 5002) - Voice Generation
```

**All models run locally. No external API calls. Full control.**

---

## üìã Prerequisites

### Hardware Requirements:

**Minimum (for testing):**
- CPU: 8+ cores
- RAM: 32GB
- GPU: NVIDIA RTX 3060 (12GB VRAM) or better
- Storage: 500GB SSD

**Recommended (for production):**
- CPU: 16+ cores
- RAM: 64GB+
- GPU: NVIDIA RTX 4090 (24GB VRAM) or A100
- Storage: 1TB+ NVMe SSD

**For multiple users:**
- Multiple GPUs (2-4x RTX 4090 or A100)
- 128GB+ RAM
- Load balancer for requests

---

## üöÄ STEP 1: Install Ollama (Local LLM)

### Installation:

**Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
Download from https://ollama.com/download/windows

### Pull Models:

```bash
# Llama 3.3 70B (best quality, needs 40GB+ VRAM)
ollama pull llama3.3:70b

# OR Llama 3.2 3B (smaller, faster, needs 2GB VRAM)
ollama pull llama3.2:3b

# OR Llama 3.2 1B (smallest, fastest, needs 1GB VRAM)
ollama pull llama3.2:1b
```

### Verify:
```bash
ollama run llama3.2:3b "Hello, how are you?"
```

### API Test:
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

---

## üöÄ STEP 2: Install ComfyUI (Local Image/Video)

### Installation:

```bash
# Clone repository
git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI

# Install dependencies
pip install -r requirements.txt

# OR use comfy-cli
pip install comfy-cli
comfy install
```

### Download Models:

```bash
# Create models directory structure
cd ComfyUI/models

# Download Flux.1-dev (image generation)
cd checkpoints/
wget https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors

# Download Stable Diffusion XL (faster alternative)
wget https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors

# Download VAE
cd ../vae/
wget https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors

# Download CogVideoX (video generation)
cd ../checkpoints/
wget https://huggingface.co/THUDM/CogVideoX-5b/resolve/main/model.safetensors
```

### Start ComfyUI:

```bash
cd ComfyUI
python main.py --listen 0.0.0.0 --port 8188
```

**Access UI:** http://localhost:8188

### API Test:

```bash
curl http://localhost:8188/system_stats
```

---

## üöÄ STEP 3: Install Coqui TTS (Local Voice)

### Installation:

```bash
# Install Coqui TTS
pip install TTS

# Test installation
tts --text "Hello, this is a test" --out_path test.wav
```

### Start API Server:

```bash
# Start TTS server
tts-server --model_name tts_models/en/ljspeech/tacotron2-DDC --port 5002
```

### API Test:

```bash
curl -X POST http://localhost:5002/api/tts \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world"}'
```

---

## üîß STEP 4: Configure Your Application

### Set Environment Variables:

```bash
# Point to your local AI server
npx convex env set LOCAL_AI_SERVER "http://YOUR_GPU_SERVER_IP:11434"
npx convex env set COMFYUI_SERVER "http://YOUR_GPU_SERVER_IP:8188"
npx convex env set COQUI_TTS_SERVER "http://YOUR_GPU_SERVER_IP:5002"

# OR if running on same machine:
npx convex env set LOCAL_AI_SERVER "http://localhost:11434"
npx convex env set COMFYUI_SERVER "http://localhost:8188"
npx convex env set COQUI_TTS_SERVER "http://localhost:5002"
```

---

## üìä Model Comparison

### Text Generation (Ollama):

| Model | Size | VRAM | Speed | Quality |
|-------|------|------|-------|---------|
| Llama 3.3 70B | 40GB | 40GB+ | Slow | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Llama 3.2 8B | 4.7GB | 8GB | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Llama 3.2 3B | 2GB | 4GB | Fast | ‚≠ê‚≠ê‚≠ê |
| Llama 3.2 1B | 1.3GB | 2GB | Very Fast | ‚≠ê‚≠ê |

### Image Generation (ComfyUI):

| Model | Size | VRAM | Speed | Quality |
|-------|------|------|-------|---------|
| Flux.1-dev | 23GB | 24GB | 30s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| SDXL Base | 6.9GB | 8GB | 10s | ‚≠ê‚≠ê‚≠ê‚≠ê |
| SD 1.5 | 2GB | 4GB | 5s | ‚≠ê‚≠ê‚≠ê |

### Video Generation (ComfyUI):

| Model | Size | VRAM | Speed | Quality |
|-------|------|------|-------|---------|
| CogVideoX-5B | 20GB | 24GB | 2-5min | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| CogVideoX-2B | 8GB | 12GB | 1-2min | ‚≠ê‚≠ê‚≠ê‚≠ê |
| AnimateDiff | 4GB | 8GB | 30s | ‚≠ê‚≠ê‚≠ê |

---

## üí∞ Cost Analysis

### Hardware Costs (One-time):

**Budget Setup ($2,000-3,000):**
- RTX 4070 Ti (12GB): $800
- 64GB RAM: $200
- CPU (Ryzen 9): $500
- Storage (1TB NVMe): $100
- Power Supply: $150
- Case, cooling, etc.: $250
- **Total**: ~$2,000

**Mid-range Setup ($4,000-6,000):**
- RTX 4090 (24GB): $1,600
- 128GB RAM: $400
- CPU (Ryzen 9 7950X): $700
- Storage (2TB NVMe): $200
- Power Supply: $200
- Case, cooling, etc.: $300
- **Total**: ~$3,400

**Production Setup ($10,000+):**
- 2x RTX 4090 or 1x A100: $3,000-10,000
- 256GB RAM: $800
- High-end CPU: $1,500
- 4TB NVMe: $400
- Everything else: $500
- **Total**: $6,200-13,200

### Operating Costs (Monthly):

**Power Consumption:**
- RTX 4090: 450W under load
- Full system: ~600W
- 24/7 operation: ~432 kWh/month
- Cost: $43-86/month (at $0.10-0.20/kWh)

**Compare to API costs:**
- 100 videos/day with Sora 2: ~$600/month
- Self-hosted pays for itself in 6-12 months

---

## üéØ Performance Expectations

### Text Generation (Llama 3.2 3B on RTX 4090):
- Speed: ~100-150 tokens/second
- Scene generation (500 tokens): 3-5 seconds
- Cost per generation: $0.00

### Image Generation (SDXL on RTX 4090):
- Speed: ~10 seconds per 1024x1024 image
- 4 images for slideshow: 40 seconds
- Cost per video: $0.00

### Video Generation (CogVideoX-2B on RTX 4090):
- Speed: 1-2 minutes per 10-second video
- Resolution: 720p-1080p
- Cost per video: $0.00

### Total Video Generation Time:
- Text (scenes): 5 seconds
- Images (4 scenes): 40 seconds
- Video (if using CogVideoX): 1-2 minutes
- Audio (TTS): 5 seconds
- **Total**: 50 seconds (slideshow) or 2.5 minutes (real video)

---

## üîí Security & Network Setup

### Firewall Rules:

```bash
# Allow local AI server access (if on separate machine)
sudo ufw allow from YOUR_APP_SERVER_IP to any port 11434
sudo ufw allow from YOUR_APP_SERVER_IP to any port 8188
sudo ufw allow from YOUR_APP_SERVER_IP to any port 5002
```

### Reverse Proxy (Optional):

Use nginx to proxy all AI services through one endpoint:

```nginx
# /etc/nginx/sites-available/ai-server
server {
    listen 80;
    server_name ai.yourdomain.com;

    location /ollama/ {
        proxy_pass http://localhost:11434/;
    }

    location /comfyui/ {
        proxy_pass http://localhost:8188/;
    }

    location /tts/ {
        proxy_pass http://localhost:5002/;
    }
}
```

---

## üìà Scaling Options

### Single GPU ‚Üí Multi-GPU:

**Option 1: Model Parallelism**
- Split large models across multiple GPUs
- Use tensor parallel or pipeline parallel
- Supported by vLLM, TGI

**Option 2: Request Parallelism**
- Each GPU handles different requests
- Use load balancer (nginx, HAProxy)
- Simple to implement

**Option 3: Dedicated GPUs**
- GPU 1: Text generation (Ollama)
- GPU 2: Image generation (ComfyUI)
- GPU 3: Video generation (CogVideoX)
- Best performance, highest cost

---

## üõ†Ô∏è Troubleshooting

### "Out of Memory" Errors:

**Solution 1: Use Smaller Models**
```bash
ollama pull llama3.2:3b  # Instead of 70b
```

**Solution 2: Enable Quantization**
```bash
ollama pull llama3.2:3b-q4_0  # 4-bit quantized
```

**Solution 3: Adjust ComfyUI Settings**
- Lower resolution (512x512 instead of 1024x1024)
- Use --lowvram flag: `python main.py --lowvram`
- Use --cpu flag for testing: `python main.py --cpu`

### Slow Generation:

**Check GPU Utilization:**
```bash
nvidia-smi -l 1
```

**Optimize Ollama:**
```bash
# Set concurrent requests
OLLAMA_NUM_PARALLEL=4 ollama serve
```

**Optimize ComfyUI:**
- Use xformers: `pip install xformers`
- Use faster sampler (DPM++ 2M Karras)
- Reduce steps (20 instead of 50)

---

## ‚úÖ Verification Checklist

After setup, verify:

- [ ] Ollama responds: `curl http://localhost:11434/api/tags`
- [ ] ComfyUI responds: `curl http://localhost:8188/system_stats`
- [ ] Coqui TTS responds: `curl http://localhost:5002/api/tts -H "Content-Type: application/json" -d '{"text":"test"}'`
- [ ] Models loaded: Check ComfyUI models folder
- [ ] GPU detected: `nvidia-smi` shows GPUs
- [ ] Generate test image in ComfyUI UI
- [ ] Generate test text with Ollama CLI

---

## üéâ Benefits of Self-Hosted

‚úÖ **No API Costs** - Pay once for hardware, use unlimited
‚úÖ **Full Privacy** - Data never leaves your servers
‚úÖ **No Rate Limits** - Generate as much as you want
‚úÖ **Full Control** - Customize models, fine-tune, optimize
‚úÖ **No Internet Required** - Works completely offline
‚úÖ **No Vendor Lock-in** - Not dependent on external services
‚úÖ **Predictable Performance** - No API downtime or throttling

---

## üìö Next Steps

1. Install Ollama and pull Llama models
2. Install ComfyUI and download checkpoints
3. Install Coqui TTS
4. Configure environment variables
5. Integrate with your application (see SELF_HOSTED_INTEGRATION.md)
6. Test end-to-end video generation
7. Monitor performance and optimize

---

**Total Setup Time:** 2-4 hours
**Total Cost:** $2,000-6,000 (one-time hardware)
**Monthly Cost:** $40-80 (electricity)
**Result:** Fully self-hosted AI with NO external dependencies

---

**Last Updated:** January 18, 2026
**Status:** Production Ready
**All models:** 100% Self-Hosted
**No fake or mock:** Real local AI generation
